{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chloe Quinto    \n",
    "CPE 695 WS   \n",
    "HW 4   \n",
    "I pledge my honor that I have abided by the Stevens Honor System - Chloe Quinto    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('./data/Titanic.csv')\n",
    "titanic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: \n",
    "\n",
    "Use our \"titanic\" dataset in homework #3 and split the data the same way you did in homework #3 as training and testing sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.dropna(how=\"all\", inplace=True) #drop all the rows where all elements are missing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\"pclass\", \"sex\", \"age\", \"sibsp\"]\n",
    "\n",
    "titanic[\"survived\"] = titanic[\"survived\"].astype(int)\n",
    "y = titanic[\"survived\"]\n",
    "x = titanic[feature_columns].apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(x,y,test_size = 0.3, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"xtrain shape: \", xtrain.shape)\n",
    "print(\"xtest shape: \", xtest.shape)\n",
    "print(\"\\n\")\n",
    "print(\"ytrain shape\", ytrain.shape)\n",
    "print(\"ytest shape\", ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 \n",
    "\n",
    "Fit a neural network using independent variables 'pclass + sex + age + sibsp' and independent variable 'survived'. Omit all NA examples. Use ***2 hidden layers*** and set the activation functions for both the hidden and output layer to be the sigmoid function. Set the \"solver\" parameter as either SGD (stochastic gradient descent) or Adam (similar to SGD but optimized performance with mini batches). You can adjust parameter \"alpha\" for regularlization (to control overfitting) and other parameters such as \"learning rate\" and \"momentum\" as needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes = (10, 10), \n",
    "                    activation=\"logistic\", \n",
    "                    solver=\"adam\", \n",
    "                    alpha =1e-5, \n",
    "                    momentum = 0.1,\n",
    "                    max_iter = 200, \n",
    "                    verbose=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the mean accuracy on the given test data and labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score100 = mlp.score(xtrain, ytrain)\n",
    "print(score100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mlp.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(ytest, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ytest, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: \n",
    "\n",
    "Check the performance of the model: in sample and out-of-sample accuracy. Please try two different network structures (i.e. number of neurons at each hideen layer) and show their respective accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In samples percent survivors/fatalities correctly predicted (on training set)\n",
    "inSample_y = mlp.predict(xtrain)\n",
    "inSampleSurvivors = 1 - sum(inSample_y[ytrain == 1] ^ ytrain[ytrain == 1]) / len(ytrain[ytrain == 1])\n",
    "inSampleFatalities = 1 - sum(inSample_y[ytrain == 0] ^ ytrain[ytrain == 0]) / len(ytrain[ytrain == 0])\n",
    "\n",
    "# out-of-samples percent survivors/fatalities correctly predicted (on training set)\n",
    "outSample_y = mlp.predict(xtest)\n",
    "outSampleSurvivors = 1 - sum(inSample_y[ytrain == 1] ^ ytrain[ytrain == 1]) / len(ytrain[ytrain == 1])\n",
    "outSampleFatalities = 1 - sum(inSample_y[ytrain == 0] ^ ytrain[ytrain == 0]) / len(ytrain[ytrain == 0])\n",
    "\n",
    "print(\"In sample percent survivors correctly predicted: {:.2f}%\".format(inSampleSurvivors*100))\n",
    "print(\"In sample percent fatalaties correctly predicted: {:.2f}%\".format(inSampleFatalities*100))\n",
    "print(\"Out sample percent survivors correctly predicted: {:.2f}%\".format(outSampleSurvivors*100))\n",
    "print(\"Out sample percent survivors correctly predicted: {:.2f}%\".format(outSampleFatalities*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Changing number of neurons in the hidden layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes = (100, 100), # 4 features + 1 label column \n",
    "                    activation=\"logistic\", \n",
    "                    solver=\"adam\", \n",
    "                    alpha =1e-5, \n",
    "                    momentum = 0.1,\n",
    "                    max_iter = 200, \n",
    "                    verbose=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score5 = mlp.score(xtrain, ytrain)\n",
    "print(score5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mlp.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(ytest, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ytest, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In samples percent survivors/fatalities correctly predicted (on training set)\n",
    "inSample_y = mlp.predict(xtrain)\n",
    "inSampleSurvivors = 1 - sum(inSample_y[ytrain == 1] ^ ytrain[ytrain == 1]) / len(ytrain[ytrain == 1])\n",
    "inSampleFatalities = 1 - sum(inSample_y[ytrain == 0] ^ ytrain[ytrain == 0]) / len(ytrain[ytrain == 0])\n",
    "\n",
    "# out-of-samples percent survivors/fatalities correctly predicted (on training set)\n",
    "outSample_y = mlp.predict(xtest)\n",
    "outSampleSurvivors = 1 - sum(inSample_y[ytrain == 1] ^ ytrain[ytrain == 1]) / len(ytrain[ytrain == 1])\n",
    "outSampleFatalities = 1 - sum(inSample_y[ytrain == 0] ^ ytrain[ytrain == 0]) / len(ytrain[ytrain == 0])\n",
    "\n",
    "print(\"In sample percent survivors correctly predicted: {:.2f}%\".format(inSampleSurvivors*100))\n",
    "print(\"In sample percent fatalaties correctly predicted: {:.2f}%\".format(inSampleFatalities*100))\n",
    "print(\"Out sample percent survivors correctly predicted: {:.2f}%\".format(outSampleSurvivors*100))\n",
    "print(\"Out sample percent survivors correctly predicted: {:.2f}%\".format(outSampleFatalities*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"A Multi-layer Perceptron with two hidden layers of 10 neurons each had an accuracy score\\n of on the given test data and labels:\",  score100)\n",
    "print(\"A Multi-layer Perceptron with two hidden layers of 100 neurons each had an accuracy score\\n of on the given test data and labels:\",  score5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "\n",
    "Compare the in-sample and out-of-sample accuracy (as defined in step 3) with the pruned decision tree obtained in homework #3 (You can either use a table of a figure to compare the accuracy of the two learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate([\n",
    "                [\"In-Sample Survivors\",  inSampleSurvivors*100 , 96.61835748792271], \n",
    "                [\"In-Sample Fatalities\",inSampleFatalities*100, 79.12552891396332], \n",
    "                [\"Out-of-Sample Survivors\", outSampleSurvivors*100, 85.22727272727273], \n",
    "                [\"Out-of-Sample Fatalitiles\", outSampleFatalities*100,74.75409836065575]], \n",
    "            headers=['Accuracy','MLP: Value', 'Pruned Tree: Value']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our results show that a pruned tree performed with a higher accuracy for determining in-sample survivors as well as out-of-sample survivors. This may be due to the fact that with MLP, we have attributes that we have to fine tune ourselves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes = (100, 100), # 4 features + 1 label column \n",
    "                    activation=\"logistic\", \n",
    "                    solver=\"adam\", \n",
    "                    alpha =1e-5, \n",
    "                    momentum = 0.1,\n",
    "                    max_iter = 200, \n",
    "                    verbose=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.coefs_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of calling MLPClassifier(), let's intuitively build what a neural network looks like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NeuralNetwork: \n",
    "#     def __init__(self): \n",
    "#         np.random.seed(10)\n",
    "import numpy as np\n",
    "\n",
    "xtrain = np.array(xtrain)\n",
    "ytrain = np.array(ytrain)\n",
    "\n",
    "ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork: \n",
    "    def __init__(self): \n",
    "        self.wij = np.random.rand(4,916) # input to hidden layer weighst\n",
    "        self.wjk = np.random.rand(916, 1) # hideen layer to output weights\n",
    "        \n",
    "    def sigmoid(self, x, w): \n",
    "        z = np.dot(x, w)\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidDerivative(self,x, w): \n",
    "        return self.sigmoid(x,w)*(1-self.sigmoid(x,w))\n",
    "    def gradientDescent(self, x, y, iterations):\n",
    "        for i in range(iterations): \n",
    "            Xi = x \n",
    "            Xj = self.sigmoid(Xi, self.wij)\n",
    "            yhat = self.sigmoid(Xj, self.wjk)\n",
    "            \n",
    "            #gradients for hidden to output weights \n",
    "            g_wjk = np.dot(Xj.T, (y-yhat)*self.sigmoidDeriative(Xj, self.wjk))\n",
    "            \n",
    "            # gradients for input to hidden weights \n",
    "            g_wij = np.dot(Xi.T, np.dot((y-yhat)*self.sigmoidDerivative(Xj, self.wjk), \n",
    "                                       self.wjk.T)* self.sigmoidDerivative(Xi, self.wij))\n",
    "            \n",
    "            # update weights \n",
    "            \n",
    "            self.wij += g_wij\n",
    "            se.f.wjk += g_wjk\n",
    "            \n",
    "     class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        np.random.seed(10) # for generating the same results\n",
    "        self.wij   = np.random.rand(3,4) # input to hidden layer weights\n",
    "        self.wjk   = np.random.rand(4,1) # hidden layer to output weights\n",
    "        \n",
    "    def sigmoid(self, x, w):\n",
    "        z = np.dot(x, w)\n",
    "        return 1/(1 + np.exp(-z))\n",
    "    \n",
    "    def sigmoid_derivative(self, x, w):\n",
    "        return self.sigmoid(x, w) * (1 - self.sigmoid(x, w))\n",
    "    \n",
    "    def gradient_descent(self, x, y, iterations):\n",
    "        for i in range(iterations):\n",
    "            Xi = x\n",
    "            Xj = self.sigmoid(Xi, self.wij)\n",
    "            yhat = self.sigmoid(Xj, self.wjk)\n",
    "            # gradients for hidden to output weights\n",
    "            g_wjk = np.dot(Xj.T, (y - yhat) * self.sigmoid_derivative(Xj, self.wjk))\n",
    "            # gradients for input to hidden weights\n",
    "            g_wij = np.dot(Xi.T, np.dot((y - yhat) * self.sigmoid_derivative(Xj, self.wjk), self.wjk.T) * self.sigmoid_derivative(Xi, self.wij))\n",
    "            # update weights\n",
    "            self.wij += g_wij\n",
    "            self.wjk += g_wjk\n",
    "        print('The final prediction from neural network are: ')\n",
    "        print(yhat)   print(\"yhat \", yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (4,3) and (4,916) not aligned: 3 (dim 1) != 4 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0cfe1d118a0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradientDescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-86bc5210d883>\u001b[0m in \u001b[0;36mgradientDescent\u001b[0;34m(self, x, y, iterations)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mXi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mXj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwij\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwjk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-86bc5210d883>\u001b[0m in \u001b[0;36msigmoid\u001b[0;34m(self, x, w)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (4,3) and (4,916) not aligned: 3 (dim 1) != 4 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "nn = NeuralNetwork()\n",
    "x = np.array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "y = np.array([[0, 1, 1, 0]]).T\n",
    "nn.gradientDescent(x,y,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [1, 1, 1],\n",
       "       [1, 0, 1],\n",
       "       [0, 1, 1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
